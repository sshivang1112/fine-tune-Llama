{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":53482,"databundleVersionId":6201832,"sourceType":"competition"},{"sourceId":9033893,"sourceType":"datasetVersion","datasetId":5445359}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sb\nfrom tqdm import tqdm, trange\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -q -U trl transformers git+https://github.com/huggingface/peft.git\n!pip install bitsandbytes\n!pip install accelerate","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nfrom trl import SFTTrainer\nfrom transformers import AutoModelForCausalLM, AutoTokenizer,\\\nBitsAndBytesConfig, AutoTokenizer\nfrom datasets import Dataset\nfrom transformers import TrainingArguments\nfrom peft import LoraConfig, get_peft_model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PATH = '/kaggle/input/combined/' \nsummaries_train = pd.read_csv(f'{PATH}/Combined.csv')\nsummaries_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summaries_train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summaries_train.nunique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\nPATH = '/kaggle/input/combined/Combined.csv'\ndf = pd.read_csv(PATH)\n\nprint(df.head())\n\ndef create_prompt_and_response(row):\n    if row['anomaly'] == 'yes':\n        prompt = f\"When did device {row['Device_id']} show an anomaly?\"\n        response = f\"The device {row['Device_id']} showed an anomaly on {row['timestamp']}.\"\n    else:\n        prompt = f\"When did device {row['Device_id']} show an anomaly?\"\n        response = f\"The device {row['Device_id']} did not show any anomaly recently.\"\n    return prompt, response\n\ndf[['prompt', 'response']] = df.apply(create_prompt_and_response, axis=1, result_type='expand')\n\nfrom datasets import Dataset\nhf_dataset = Dataset.from_pandas(df[['prompt', 'response']])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name = \"TinyPixel/Llama-2-7B-bf16-sharded\"\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    trust_remote_code=True\n)\n\nmodel.config.use_cache = False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_name, \n                                          trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lora_alpha = 16\nlora_dropout = 0.1\nlora_r = 64\n\npeft_config = LoraConfig(\n    lora_alpha=lora_alpha,\n    lora_dropout=lora_dropout,\n    r=lora_r,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import TrainingArguments\n\noutput_dir = \"./results\"\nper_device_train_batch_size = 2\ngradient_accumulation_steps = 1\noptim = \"paged_adamw_32bit\"\nsave_steps = 50\nlogging_steps = 5\nlearning_rate = 2e-4\nmax_grad_norm = 0.3\nnum_train_epochs = 0.001 # Number of epochs\nmax_steps = None  # Steps, not epochs\nwarmup_ratio = 0.03\nlr_scheduler_type = \"constant\"\n\nmax_seq_length = 1024\n\ntraining_arguments = TrainingArguments(\n    output_dir=output_dir,\n    per_device_train_batch_size=per_device_train_batch_size,\n    gradient_accumulation_steps=gradient_accumulation_steps,\n    optim=optim,\n    save_steps=save_steps,\n    logging_steps=logging_steps,\n    learning_rate=learning_rate,\n    fp16=True,\n    max_grad_norm=max_grad_norm,\n    num_train_epochs=num_train_epochs,  # Set number of epochs\n    warmup_ratio=warmup_ratio,\n    group_by_length=True,\n    lr_scheduler_type=lr_scheduler_type,\n)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_metric\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    preds = predictions.argmax(-1)\n    metric = load_metric(\"accuracy\")\n    return metric.compute(predictions=preds, references=labels)\n\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=hf_dataset,\n    eval_dataset=hf_dataset,  # Add an evaluation dataset\n    peft_config=peft_config,\n    dataset_text_field=\"prompt\",\n    max_seq_length=max_seq_length,\n    tokenizer=tokenizer,\n    args=training_arguments,\n    compute_metrics=compute_metrics  # Include the compute_metrics function\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for name, module in trainer.model.named_modules():\n    if \"norm\" in name:\n        module = module.to(torch.float32)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_to_save = trainer.model.module if hasattr(trainer.model, 'module') else trainer.model\nmodel_to_save.save_pretrained(\"final_finetuned_model\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import pipeline\n\npipe = pipeline(\"text-generation\", model=\"final_finetuned_model\", tokenizer=tokenizer)\n\nprompt = \"Does  device f66f1a4a-e7db-41d4-aec8-f7392ce5ab11 show an anomaly on 2023-01-11 \"\nresponse = pipe(prompt, max_length=50)\nprint(response[0]['generated_text'])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}